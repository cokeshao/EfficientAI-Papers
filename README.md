# EfficientAI Papers

---

### PTQ

#### [Q0] LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale
- **🧑‍🔬 Main Author**: Tim Dettmers
- **🏫 Affifiation**: University of Washington
- **🔗 Links**: [[Paper](https://arxiv.org/abs/2208.07339)] [[Code](https://github.com/bitsandbytes-foundation/bitsandbytes)]
- **📝 Note**: NeurIPS 2022; W8A8

#### [Q1] Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models
- **🧑‍🔬 Main Author**: Xiuying Wei
- **🏫 Affifiation**: Beihang University
- **🔗 Links**: [[Paper](https://arxiv.org/abs/2209.13325)] [[Code](https://github.com/wimh966/outlier_suppression)]
- **📝 Note**: NeurIPS 2022; W8A8

#### [Q2] GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers
- **🧑‍🔬 Main Author**: Elias Frantar
- **🏫 Affifiation**: IST Austria
- **🔗 Links**: [[Paper](https://arxiv.org/pdf/2210.17323)] [[Code](https://github.com/IST-DASLab/gptq)]
- **📝 Note**: ICLR 2023; W4A16

#### [Q3] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models
- **🧑‍🔬 Main Author**: Guangxuan Xiao, Ji Lin; Song Han
- **🏫 Affifiation**: MIT
- **🔗 Links**: [[Paper](https://arxiv.org/abs/2211.10438)] [[Code](https://github.com/mit-han-lab/smoothquant)]
- **📝 Note**: ICML 2023; W8A8

#### [Q4] RPTQ: Reorder-based Post-training Quantization for Large Language Models
- **🧑‍🔬 Main Author**: Zhihang Yuan
- **🏫 Affifiation**: Houmo AI
- **🔗 Links**: [[Paper](https://arxiv.org/abs/2304.01089)] [[Code](https://github.com/hahnyuan/RPTQ4LLM)]
- **📝 Note**: W4A4/W4A8/W4A4KV

#### [Q5] OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization
- **🧑‍🔬 Main Author**: Cong Guo, Jiaming Tang
- **🏫 Affifiation**: Shanghai Jiao Tong University
- **🔗 Links**: [[Paper](https://arxiv.org/abs/2304.07493)] [[Code]()]
- **📝 Note**: ISCA 2023; W4A4

#### [Q6] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration
- **🧑‍🔬 Main Author**: Ji Lin, Jiaming Tang; Song Han
- **🏫 Affifiation**: MIT
- **🔗 Links**: [[Paper](https://arxiv.org/abs/2306.00978)] [[Code](https://github.com/mit-han-lab/llm-awq)]
- **📝 Note**: 🔥 MLSys 2024 Best Paper Award; W4A16

#### [Q7] OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models
- **🧑‍🔬 Main Author**: Changhun Lee, Jungyu Jin
- **🏫 Affifiation**: POSTECH
- **🔗 Links**: [[Paper](https://arxiv.org/abs/2306.02272)] [[Code](https://github.com/xvyaward/owq)]
- **📝 Note**: AAAI 2024 (oral); W4A16

#### [Q8] SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression
- **🧑‍🔬 Main Author**: Tim Dettmers
- **🏫 Affifiation**: University of Washington
- **🔗 Links**: [[Paper](https://arxiv.org/abs/2306.03078)] [[Code](https://github.com/Vahe1994/SpQR)]
- **📝 Note**: W4A16

#### [Q9] SqueezeLLM: Dense-and-Sparse Quantization
- **🧑‍🔬 Main Author**: Sehoon Kim
- **🏫 Affifiation**: UC Berkeley
- **🔗 Links**: [[Paper](https://arxiv.org/abs/2306.07629)] [[Code](https://github.com/SqueezeAILab/SqueezeLLM)]
- **📝 Note**: ICML 2024; W4A16

#### [Q10] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models
- **🧑‍🔬 Main Author**: Wenqi Shao, Mengzhao Chen
- **🏫 Affifiation**: Shanghai AI Laboratory
- **🔗 Links**: [[Paper](https://arxiv.org/abs/2308.13137)] [[Code](https://github.com/OpenGVLab/OmniQuant)]
- **📝 Note**: NeurIPS 2022; W4A4

#### [Q11] Atom: Low-bit Quantization for Efficient and Accurate LLM Serving
- **🧑‍🔬 Main Author**: Yilong Zhao
- **🏫 Affifiation**: University of Washington
- **🔗 Links**: [[Paper](https://arxiv.org/abs/2310.19102)] [[Code](https://github.com/efeslab/Atom)]
- **📝 Note**: MLSys 2024; W4A4



