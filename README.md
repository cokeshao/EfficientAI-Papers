# EfficientAI Papers

---

### PTQ

#### [Q0] LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale
- **ğŸ§‘â€ğŸ”¬ Main Author**: Tim Dettmers
- **ğŸ« Affifiation**: University of Washington
- **ğŸ”— Links**: [[Paper](https://arxiv.org/abs/2208.07339)] [[Code](https://github.com/bitsandbytes-foundation/bitsandbytes)]
- **ğŸ“ Note**: NeurIPS 2022; W8A8

#### [Q1] Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models
- **ğŸ§‘â€ğŸ”¬ Main Author**: Xiuying Wei
- **ğŸ« Affifiation**: Beihang University
- **ğŸ”— Links**: [[Paper](https://arxiv.org/abs/2209.13325)] [[Code](https://github.com/wimh966/outlier_suppression)]
- **ğŸ“ Note**: NeurIPS 2022; W8A8

#### [Q2] GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers
- **ğŸ§‘â€ğŸ”¬ Main Author**: Elias Frantar
- **ğŸ« Affifiation**: IST Austria
- **ğŸ”— Links**: [[Paper](https://arxiv.org/pdf/2210.17323)] [[Code](https://github.com/IST-DASLab/gptq)]
- **ğŸ“ Note**: ICLR 2023; W4A16

#### [Q3] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models
- **ğŸ§‘â€ğŸ”¬ Main Author**: Guangxuan Xiao, Ji Lin; Song Han
- **ğŸ« Affifiation**: MIT
- **ğŸ”— Links**: [[Paper](https://arxiv.org/abs/2211.10438)] [[Code](https://github.com/mit-han-lab/smoothquant)]
- **ğŸ“ Note**: ICML 2023; W8A8

#### [Q4] RPTQ: Reorder-based Post-training Quantization for Large Language Models
- **ğŸ§‘â€ğŸ”¬ Main Author**: Zhihang Yuan
- **ğŸ« Affifiation**: Houmo AI
- **ğŸ”— Links**: [[Paper](https://arxiv.org/abs/2304.01089)] [[Code](https://github.com/hahnyuan/RPTQ4LLM)]
- **ğŸ“ Note**: W4A4/W4A8/W4A4KV

#### [Q5] OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization
- **ğŸ§‘â€ğŸ”¬ Main Author**: Cong Guo, Jiaming Tang
- **ğŸ« Affifiation**: Shanghai Jiao Tong University
- **ğŸ”— Links**: [[Paper](https://arxiv.org/abs/2304.07493)] [[Code]()]
- **ğŸ“ Note**: ISCA 2023; W4A4

#### [Q6] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration
- **ğŸ§‘â€ğŸ”¬ Main Author**: Ji Lin, Jiaming Tang; Song Han
- **ğŸ« Affifiation**: MIT
- **ğŸ”— Links**: [[Paper](https://arxiv.org/abs/2306.00978)] [[Code](https://github.com/mit-han-lab/llm-awq)]
- **ğŸ“ Note**: ğŸ”¥Â MLSys 2024 Best Paper Award; W4A16

#### [Q7] OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models
- **ğŸ§‘â€ğŸ”¬ Main Author**: Changhun Lee, Jungyu Jin
- **ğŸ« Affifiation**: POSTECH
- **ğŸ”— Links**: [[Paper](https://arxiv.org/abs/2306.02272)] [[Code](https://github.com/xvyaward/owq)]
- **ğŸ“ Note**: AAAI 2024 (oral); W4A16

#### [Q8] SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression
- **ğŸ§‘â€ğŸ”¬ Main Author**: Tim Dettmers
- **ğŸ« Affifiation**: University of Washington
- **ğŸ”— Links**: [[Paper](https://arxiv.org/abs/2306.03078)] [[Code](https://github.com/Vahe1994/SpQR)]
- **ğŸ“ Note**: W4A16

#### [Q9] SqueezeLLM: Dense-and-Sparse Quantization
- **ğŸ§‘â€ğŸ”¬ Main Author**: Sehoon Kim
- **ğŸ« Affifiation**: UC Berkeley
- **ğŸ”— Links**: [[Paper](https://arxiv.org/abs/2306.07629)] [[Code](https://github.com/SqueezeAILab/SqueezeLLM)]
- **ğŸ“ Note**: ICML 2024; W4A16

#### [Q10] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models
- **ğŸ§‘â€ğŸ”¬ Main Author**: Wenqi Shao, Mengzhao Chen
- **ğŸ« Affifiation**: Shanghai AI Laboratory
- **ğŸ”— Links**: [[Paper](https://arxiv.org/abs/2308.13137)] [[Code](https://github.com/OpenGVLab/OmniQuant)]
- **ğŸ“ Note**: NeurIPS 2022; W4A4

#### [Q11] Atom: Low-bit Quantization for Efficient and Accurate LLM Serving
- **ğŸ§‘â€ğŸ”¬ Main Author**: Yilong Zhao
- **ğŸ« Affifiation**: University of Washington
- **ğŸ”— Links**: [[Paper](https://arxiv.org/abs/2310.19102)] [[Code](https://github.com/efeslab/Atom)]
- **ğŸ“ Note**: MLSys 2024; W4A4



